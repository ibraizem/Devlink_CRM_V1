// app/fichiers/services/fileService.ts
import { supabase } from '@/lib/supabase/client';
import { FichierImport } from '@/lib/types/fichier';
import { SheetInfo } from '@/lib/types/file.types';
import * as XLSX from 'xlsx';
import { v4 as uuidv4 } from 'uuid';

const BUCKET_NAME = 'fichiers';

// V√©rifier et cr√©er le dossier utilisateur dans le bucket
const ensureBucketExists = async (userId: string) => {
  try {
    // V√©rifier si le bucket existe en essayant de lister ses fichiers
    const { error: bucketCheckError } = await supabase.storage
      .from(BUCKET_NAME)
      .list();
      
    if (bucketCheckError) {
      console.error('Erreur d\'acc√®s au bucket. V√©rifiez qu\'il existe et que les politiques RLS sont correctement configur√©es:', bucketCheckError);
      throw new Error('Impossible d\'acc√©der au bucket. V√©rifiez votre connexion et les permissions.');
    }

    // Cr√©er un dossier pour l'utilisateur s'il n'existe pas
    const userFolderPath = `${userId}/`;
    
    // V√©rifier d'abord si le dossier existe d√©j√†
    const { data: existingFolders } = await supabase.storage
      .from(BUCKET_NAME)
      .list(userId);
      
    // Si le dossier n'existe pas, on le cr√©e
    if (!existingFolders) {
      const { error: folderError } = await supabase.storage
        .from(BUCKET_NAME)
        .upload(userFolderPath, new Blob(), {
          upsert: true
        });

      if (folderError && !folderError.message.includes('already exists')) {
        console.error('Erreur lors de la cr√©ation du dossier utilisateur:', folderError);
        throw new Error('Impossible de cr√©er le dossier utilisateur.');
      }
    }

    return true;
  } catch (error) {
    console.error('Erreur dans ensureBucketExists:', error);
    throw error;
  }
};

// Types
type FileUploadOptions = {
  onProgress?: (progress: number) => void;
  abortSignal?: AbortSignal;
  user_id: string;
};

type FileFilterOptions = {
  status?: 'actif' | 'inactif' | 'en_cours' | 'erreur' | 'all';
  fileType?: 'spreadsheet' | 'document' | 'image' | 'all';
  dateRange?: { start: Date; end: Date };
  searchQuery?: string;
  page?: number;
  pageSize?: number;
};

// D√©finition du type pour les colonnes personnalis√©es
export interface CustomColumn {
  id: string;
  user_id: string;
  column_name: string;
  display_name: string;
  created_at: string;
  updated_at: string;
}

export const fileService = {
  // ====================================
  // IMPORT MULTICANAL AVEC D√âTECTION AUTOMATIQUE
  // ====================================

  /**
   * Import multicanal avec d√©tection automatique des sources et cat√©gorisation
   */
  importMultiChannel: async (file: File, options: {
    detectChannel?: boolean;
    autoCategorize?: boolean;
    enrichFromExternal?: boolean;
    userId: string;
    onProgress?: (progress: number) => void;
  }): Promise<{
    channels: string[];
    categorizedLeads: any[];
    qualityScore: number;
    importSummary: {
      totalLeads: number;
      validLeads: number;
      invalidLeads: number;
      duplicates: number;
      categories: Record<string, number>;
    };
  }> => {
    try {
      console.log('üöÄ D√©but de l\'import multicanal...');
      
      // 1. Lire et analyser le fichier
      const fileData = await fileService.readFileHeaders(file);
      if (!fileData || fileData.length === 0) {
        throw new Error('Impossible de lire le fichier ou fichier vide');
      }

      const sheet = fileData[0];
      const headers = sheet.headers;
      
      // Lire les donn√©es compl√®tes du fichier
      const data = await fileService.readFileAsJson(file, 0);
      if (!data || data.length <= 1) { // La premi√®re ligne contient les en-t√™tes
        throw new Error('Fichier vide ou sans donn√©es valides');
      }
      
      // Extraire les donn√©es (exclure la ligne d'en-t√™tes)
      const rowData = data.slice(1);

      // 2. D√©tection automatique des canaux
      const detectedChannels = options.detectChannel 
        ? await fileService.detectChannels(headers, rowData)
        : ['email']; // Par d√©faut

      console.log('üì° Canaux d√©tect√©s:', detectedChannels);

      // 3. Cat√©gorisation automatique des leads
      const categorizedLeads = options.autoCategorize
        ? await fileService.categorizeLeads(rowData, headers, detectedChannels)
        : rowData.map((row, index) => ({ id: index, data: row, category: 'non_cat√©goris√©', channels: detectedChannels }));

      // 4. Validation et nettoyage des donn√©es
      const { validLeads, invalidLeads, duplicates } = await fileService.validateAndCleanLeads(
        categorizedLeads,
        headers
      );

      // 5. Calcul du score de qualit√©
      const qualityScore = fileService.calculateQualityScore(validLeads ?? [], invalidLeads ?? [], duplicates ?? []);

      // 6. Enrichissement externe si demand√©
      if (options.enrichFromExternal && validLeads) {
        await fileService.enrichLeadsFromExternal(validLeads);
      }

      // 7. Cr√©er le fichier avec les m√©tadonn√©es multicanal
      const fileRecord = await fileService.uploadFile(file, {
        user_id: options.userId,
        onProgress: options.onProgress
      });

      // 8. Mettre √† jour les m√©tadonn√©es avec les informations multicanal
      if (fileRecord?.id) {
        await fileService.updateFileMetadata(fileRecord.id, {
          channels: detectedChannels,
          categories: fileService.getCategoryStats(categorizedLeads),
          qualityScore,
          importType: 'multichannel'
        });
      }

      // 9. Ins√©rer les leads valides avec les informations de canal
      if (validLeads && fileRecord?.id) {
        await fileService.insertLeadsWithChannelInfo(validLeads, fileRecord.id, detectedChannels);
      }

      const importSummary = {
        totalLeads: data.length,
        validLeads: validLeads?.length ?? 0,
        invalidLeads: invalidLeads?.length ?? 0,
        duplicates: duplicates?.length ?? 0,
        categories: fileService.getCategoryStats(categorizedLeads)
      };

      console.log('‚úÖ Import multicanal termin√©:', importSummary);

      return {
        channels: detectedChannels,
        categorizedLeads: validLeads ?? [],
        qualityScore,
        importSummary
      };

    } catch (error) {
      console.error('‚ùå Erreur lors de l\'import multicanal:', error);
      throw error;
    }
  },

  /**
   * D√©tecte automatiquement les canaux de communication disponibles
   */
  detectChannels: async (headers: string[], data: any[]): Promise<string[]> => {
    const channels = new Set<string>();
    
    // D√©tection bas√©e sur les en-t√™tes
    const channelPatterns = {
      email: ['email', 'mail', 'e-mail', 'adresse email', 'courriel'],
      phone: ['telephone', 'tel', 'phone', 'portable', 'mobile', 't√©l√©phone'],
      linkedin: ['linkedin', 'linked in', 'profile', 'profil'],
      website: ['website', 'site', 'url', 'web', 'site web'],
      sms: ['sms', 'text', 'portable', 'mobile'],
      whatsapp: ['whatsapp', 'whatsapp number'],
      facebook: ['facebook', 'fb', 'social'],
      instagram: ['instagram', 'ig', 'insta'],
      twitter: ['twitter', 'x', 'tweet']
    };

    // Normaliser les en-t√™tes pour la d√©tection
    const normalizedHeaders = headers.map(h => h.toLowerCase().trim());
    
    normalizedHeaders.forEach((header, index) => {
      Object.entries(channelPatterns).forEach(([channel, patterns]) => {
        if (patterns.some(pattern => header.includes(pattern))) {
          channels.add(channel);
        }
      });
    });

    // D√©tection bas√©e sur les donn√©es (v√©rifier s'il y a des valeurs valides)
    if (data.length > 0) {
      Object.entries(channelPatterns).forEach(([channel, patterns]) => {
        const headerIndex = normalizedHeaders.findIndex(h => 
          patterns.some(pattern => h.includes(pattern))
        );
        
        if (headerIndex !== -1) {
          // V√©rifier si les donn√©es dans cette colonne semblent valides
          const sampleData = data.slice(0, 10).map(row => row[headerIndex]).filter(Boolean);
          
          if (sampleData.length > 0) {
            // Validation basique selon le type de canal
            const isValid = fileService.validateChannelData?.(channel, sampleData);
            if (isValid) {
              channels.add(channel);
            }
          }
        }
      });
    }

    return Array.from(channels);
  },

  /**
   * Valide les donn√©es d'un canal sp√©cifique
   */
  validateChannelData: (channel: string, sampleData: string[]): boolean => {
    const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
    const phoneRegex = /^[\d\s\-\+\(\)]+$/;
    const urlRegex = /^https?:\/\/.+/i;

    switch (channel) {
      case 'email':
        return sampleData.some(data => emailRegex.test(data));
      case 'phone':
      case 'sms':
      case 'whatsapp':
        return sampleData.some(data => phoneRegex.test(data) && data.length >= 10);
      case 'linkedin':
      case 'facebook':
      case 'instagram':
      case 'twitter':
        return sampleData.some(data => 
          data.toLowerCase().includes(channel) || 
          urlRegex.test(data)
        );
      case 'website':
        return sampleData.some(data => urlRegex.test(data));
      default:
        return sampleData.length > 0;
    }
  },

  /**
   * Cat√©gorise automatiquement les leads selon leur profil
   */
  categorizeLeads: async (data: any[], headers: string[], channels: string[]): Promise<any[]> => {
    return data.map(row => {
      const category = fileService.determineLeadCategory?.(row, headers, channels) ?? 'general';
      const confidence = fileService.calculateCategoryConfidence?.(row, headers, category) ?? 0.5;
      
      return {
        data: row,
        category,
        confidence,
        channels: fileService.extractLeadChannels?.(row, headers, channels) ?? [],
        priority: fileService.determineLeadPriority?.(row, headers, category) ?? 'low',
        metadata: {
          originalRow: row,
          categorization: {
            category,
            confidence,
            reasoning: fileService.getCategoryReasoning?.(row, headers, category) ?? 'Cat√©gorisation par d√©faut'
          }
        }
      };
    });
  },

  /**
   * D√©termine la cat√©gorie d'un lead
   */
  determineLeadCategory: (row: any[], headers: string[], channels: string[]): string => {
    const normalizedHeaders = headers.map(h => h.toLowerCase().trim());
    const normalizedRow = row.map(cell => String(cell || '').toLowerCase().trim());
    
    // Cat√©gories B2B
    const b2bIndicators = ['entreprise', 'company', 'soci√©t√©', 'sarl', 'sas', 'eurl', 'sa', 'business'];
    const hasB2BIndicators = normalizedRow.some(cell => 
      b2bIndicators.some(indicator => cell.includes(indicator))
    );
    
    if (hasB2BIndicators) {
      return 'b2b';
    }

    // Cat√©gories par canal disponible
    if (channels.includes('linkedin') && fileService.hasValidLinkedIn(row, headers)) {
      return 'professional';
    }
    
    if (channels.includes('phone') && channels.includes('email')) {
      return 'multicanal';
    }
    
    if (channels.includes('email')) {
      return 'digital';
    }

    // Cat√©gories par fonction/poste
    const jobTitles = ['directeur', 'manager', 'chef', 'responsable', 'pdg', 'ceo', 'cto', 'directrice'];
    const hasJobTitle = normalizedRow.some(cell => 
      jobTitles.some(title => cell.includes(title))
    );
    
    if (hasJobTitle) {
      return 'decision_maker';
    }

    return 'general';
  },

  /**
   * V√©rifie si un lead a un LinkedIn valide
   */
  hasValidLinkedIn: (row: any[], headers: string[]): boolean => {
    const linkedinIndex = headers.findIndex(h => 
      h.toLowerCase().includes('linkedin')
    );
    
    if (linkedinIndex !== -1 && row[linkedinIndex]) {
      const linkedinValue = String(row[linkedinIndex]).toLowerCase();
      return linkedinValue.includes('linkedin.com') || linkedinValue.includes('linkedin');
    }
    
    return false;
  },

  /**
   * Calcule la confiance de la cat√©gorisation
   */
  calculateCategoryConfidence: (row: any[], headers: string[], category: string): number => {
    let confidence = 0.5; // Base
    
    // Plus de donn√©es disponibles = plus de confiance
    const nonEmptyCells = row.filter(cell => cell && String(cell).trim() !== '').length;
    confidence += (nonEmptyCells / headers.length) * 0.3;
    
    // Certains canaux augmentent la confiance
    const hasEmail = headers.some((h, i) => 
      h.toLowerCase().includes('email') && row[i] && String(row[i]).includes('@')
    );
    const hasPhone = headers.some((h, i) => 
      h.toLowerCase().includes('tel') && row[i] && String(row[i]).length >= 10
    );
    
    if (hasEmail) confidence += 0.1;
    if (hasPhone) confidence += 0.1;
    
    return Math.min(confidence, 1.0);
  },

  /**
   * Extrait les canaux disponibles pour un lead
   */
  extractLeadChannels: (row: any[], headers: string[], availableChannels: string[]): string[] => {
    const leadChannels: string[] = [];
    
    availableChannels.forEach(channel => {
      const hasChannel = headers.some((header, index) => {
        const headerLower = header.toLowerCase();
        const cellValue = row[index];
        
        switch (channel) {
          case 'email':
            return headerLower.includes('email') && cellValue && String(cellValue).includes('@');
          case 'phone':
          case 'sms':
          case 'whatsapp':
            return headerLower.includes('tel') && cellValue && String(cellValue).length >= 10;
          case 'linkedin':
            return headerLower.includes('linkedin') && cellValue && String(cellValue).toLowerCase().includes('linkedin');
          case 'website':
            return headerLower.includes('website') && cellValue && String(cellValue).includes('http');
          default:
            return false;
        }
      });
      
      if (hasChannel) {
        leadChannels.push(channel);
      }
    });
    
    return leadChannels;
  },

  /**
   * D√©termine la priorit√© d'un lead
   */
  determineLeadPriority: (row: any[], headers: string[], category: string): 'high' | 'medium' | 'low' => {
    let score = 0;
    
    // Score par cat√©gorie
    type CategoryType = 'b2b' | 'decision_maker' | 'multicanal' | 'professional' | 'digital' | 'general';
    const categoryScores: Record<CategoryType, number> = {
      'b2b': 3,
      'decision_maker': 3,
      'multicanal': 3,
      'professional': 2,
      'digital': 1,
      'general': 0
    };
    
    score += categoryScores[category as CategoryType] || 0;
    
    // Score par nombre de canaux
    const nonEmptyCells = row.filter(cell => cell && String(cell).trim() !== '').length;
    score += Math.min(nonEmptyCells / 3, 2);
    
    if (score >= 4) return 'high';
    if (score >= 2) return 'medium';
    return 'low';
  },

  /**
   * Retourne la raison de la cat√©gorisation
   */
  getCategoryReasoning: (row: any[], headers: string[], category: string): string => {
    type CategoryType = 'b2b' | 'decision_maker' | 'multicanal' | 'professional' | 'digital' | 'general';
    const reasons: Record<CategoryType, string> = {
      'b2b': 'Pr√©sence d\'indicateurs d\'entreprise (SAS, SARL, etc.)',
      'decision_maker': 'Pr√©sence de titres de direction (PDG, Directeur, etc.)',
      'multicanal': 'Disponibilit√© de plusieurs canaux de contact',
      'professional': 'Profil LinkedIn d√©tect√©',
      'digital': 'Contact principalement par email',
      'general': 'Profil standard sans caract√©ristiques particuli√®res'
    };
    
    return reasons[category as CategoryType] || 'Cat√©gorisation par d√©faut';
  },

  /**
   * Valide et nettoie les leads
   */
  validateAndCleanLeads: async (categorizedLeads: any[], headers: string[]): Promise<{
    validLeads: any[];
    invalidLeads: any[];
    duplicates: any[];
  }> => {
    const validLeads: any[] = [];
    const invalidLeads: any[] = [];
    const duplicates: any[] = [];
    const seenEmails = new Set();
    const seenPhones = new Set();
    
    categorizedLeads.forEach(lead => {
      const email = fileService.extractValue(lead.data, headers, 'email');
      const phone = fileService.extractValue(lead.data, headers, 'tel');
      
      // V√©rification des doublons
      if (email && seenEmails.has(email.toLowerCase())) {
        duplicates.push(lead);
        return;
      }
      if (phone && seenPhones.has(phone.replace(/\s/g, ''))) {
        duplicates.push(lead);
        return;
      }
      
      // Validation minimale
      const hasValidContact = (email && fileService.isValidEmail(email)) || 
                             (phone && fileService.isValidPhone(phone));
      
      if (hasValidContact) {
        validLeads.push(lead);
        if (email) seenEmails.add(email.toLowerCase());
        if (phone) seenPhones.add(phone.replace(/\s/g, ''));
      } else {
        invalidLeads.push(lead);
      }
    });
    
    return { validLeads, invalidLeads, duplicates };
  },

  /**
   * Extrait une valeur selon le type de colonne
   */
  extractValue: (row: any[], headers: string[], type: string): string => {
    const index = headers.findIndex(h => h.toLowerCase().includes(type));
    return index !== -1 ? String(row[index] || '').trim() : '';
  },

  /**
   * Valide un email
   */
  isValidEmail: (email: string): boolean => {
    const emailRegex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
    return emailRegex.test(email);
  },

  /**
   * Valide un num√©ro de t√©l√©phone
   */
  isValidPhone: (phone: string): boolean => {
    const phoneRegex = /^[\d\s\-\+\(\)]+$/;
    return phoneRegex.test(phone) && phone.replace(/\D/g, '').length >= 10;
  },

  /**
   * Calcule le score de qualit√© global
   */
  calculateQualityScore: (validLeads: any[], invalidLeads: any[], duplicates: any[]): number => {
    const total = validLeads.length + invalidLeads.length + duplicates.length;
    if (total === 0) return 0;
    
    const validRatio = validLeads.length / total;
    const duplicatePenalty = duplicates.length / total * 0.5;
    
    return Math.max(0, Math.min(1, validRatio - duplicatePenalty));
  },

  /**
   * Enrichit les leads depuis des sources externes (placeholder)
   */
  enrichLeadsFromExternal: async (leads: any[]): Promise<void> => {
    // TODO: Impl√©menter l'enrichissement depuis des APIs externes
    // - Clearbit, Hunter.io, etc.
    console.log('üîÑ Enrichissement externe (non impl√©ment√©)');
  },

  /**
   * Met √† jour les m√©tadonn√©es du fichier avec les informations multicanal
   */
  updateFileMetadata: async (fileId: string, metadata: {
    channels: string[];
    categories: Record<string, number>;
    qualityScore: number;
    importType: string;
  }): Promise<void> => {
    try {
      const { error } = await supabase
        .from('fichiers_import')
        .update({
          metadata: metadata,
          updated_at: new Date().toISOString()
        })
        .eq('id', fileId);
      
      if (error) throw error;
    } catch (error) {
      console.error('Erreur lors de la mise √† jour des m√©tadonn√©es:', error);
    }
  },

  /**
   * Ins√®re les leads avec les informations de canal
   */
  insertLeadsWithChannelInfo: async (leads: any[], fileId: string, channels: string[]): Promise<void> => {
    try {
      const leadsToInsert = leads.map(lead => ({
        id: crypto.randomUUID(),
        nom: lead.data?.[0] || 'Inconnu',
        prenom: lead.data?.[1] || '',
        email: fileService.extractValue(lead.data, ['email', 'mail'], 'email'),
        telephone: fileService.extractValue(lead.data, ['tel', 'phone'], 'tel'),
        statut: 'nouveau',
        campaign_id: null,
        agent_id: null, // Sera d√©fini plus tard
        fichier_id: fileId,
        created_at: new Date().toISOString(),
        updated_at: new Date().toISOString(),
        source_import: 'multichannel',
        metadata: {
          category: lead.category,
          confidence: lead.confidence,
          priority: lead.priority,
          channels: lead.channels,
          channels_available: channels
        }
      }));

      const { error } = await supabase
        .from('leads')
        .insert(leadsToInsert);
      
      if (error) throw error;
    } catch (error) {
      console.error('Erreur lors de l\'insertion des leads:', error);
      throw error;
    }
  },

  /**
   * Calcule les statistiques par cat√©gorie
   */
  getCategoryStats: (categorizedLeads: any[]): Record<string, number> => {
    const stats: Record<string, number> = {};
    
    categorizedLeads.forEach(lead => {
      const category = lead.category || 'unknown';
      stats[category] = (stats[category] || 0) + 1;
    });
    
    return stats;
  },

  // ====================================
  // Gestion des fichiers
  // ====================================

  /**
   * R√©cup√®re la liste des fichiers avec filtrage et pagination
   */
  getFiles: async (
    userId: string,
    filters: FileFilterOptions = {},
    page = 1,
    pageSize = 20
  ): Promise<{ data: FichierImport[]; count: number }> => {
    try {
      let query = supabase
        .from('fichiers_import')
        .select('*', { count: 'exact' })
        .eq('user_id', userId);

      if (filters.status && filters.status !== 'all') {
        query = query.eq('statut', filters.status);
      }

      if (filters.searchQuery) {
        query = query.ilike('original_filename', `%${filters.searchQuery}%`);
      }

      if (filters.dateRange) {
        query = query
          .gte('created_at', filters.dateRange.start.toISOString())
          .lte('created_at', filters.dateRange.end.toISOString());
      }

      const from = (page - 1) * pageSize;
      const to = from + pageSize - 1;
      query = query.range(from, to).order('created_at', { ascending: false });

      const { data, error, count } = await query;

      if (error) {
        console.error('Erreur lors du chargement des fichiers:', error);
        throw new Error('Erreur lors du chargement des fichiers');
      }

      return { data: data || [], count: count || 0 };
    } catch (error) {
      console.error('Erreur dans getFiles:', error);
      throw error;
    }
  },

  /**
   * T√©l√©verse un fichier avec suivi de progression
   */
  uploadFile: async (file: File, options: FileUploadOptions): Promise<FichierImport> => {
    const { user_id, onProgress } = options;
    const fileExt = file.name.split('.').pop();
    const fileName = `${uuidv4()}.${fileExt}`;
    
    // Utiliser le dossier utilisateur pour le stockage
    const filePath = `${user_id}/${fileName}`;

    try {
      // S'assurer que le bucket et le dossier utilisateur existent
      await ensureBucketExists(user_id);

      const MAX_FILE_SIZE = 50 * 1024 * 1024; // 50MB
      if (file.size > MAX_FILE_SIZE) {
        throw new Error(`La taille du fichier d√©passe la limite de ${MAX_FILE_SIZE / (1024 * 1024)}MB`);
      }

      const allowedTypes = [
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        'application/vnd.ms-excel',
        'text/csv',
      ];

      if (!allowedTypes.includes(file.type)) {
        throw new Error('Type de fichier non pris en charge. Veuillez t√©l√©charger un fichier Excel ou CSV.');
      }

      // T√©l√©verser le fichier avec suivi de progression
      const { data: uploadData, error: uploadError } = await supabase.storage
        .from(BUCKET_NAME)
        .upload(filePath, file, {
          cacheControl: '3600',
          upsert: false,
          contentType: file.type,
        });

      if (uploadError) {
        console.error('Erreur de t√©l√©versement:', uploadError);
        throw new Error(`√âchec du t√©l√©versement: ${uploadError.message}`);
      }

      // Cr√©ation de l'entr√©e DB - en respectant la structure de la table fichiers_import
      const fileRecord = await fileService.createFileRecord({
        // Champs obligatoires de la table
        nom: file.name,
        chemin: filePath,
        statut: 'actif',
        date_import: new Date().toISOString(),
        nb_lignes: 0,
        nb_lignes_importees: 0,
        mapping_colonnes: {},
        separateur: ',',
        user_id,
        // Champs suppl√©mentaires requis par le type FichierImport
        chemin_fichier: filePath,
        mime_type: file.type,
        original_filename: file.name,
        taille: file.size,
        type: file.type,
      });

      return fileRecord;
    } catch (error) {
      console.error('Erreur lors du t√©l√©versement du fichier:', error);
      try {
        await supabase.storage.from(BUCKET_NAME).remove([filePath]);
      } catch (cleanupError) {
        console.error('Erreur lors du nettoyage du fichier:', cleanupError);
      }
      throw error instanceof Error ? error : new Error('Une erreur inconnue est survenue lors du t√©l√©chargement');
    }
  },

  /**
   * Cr√©e une entr√©e de fichier dans la base de donn√©es
   */
  createFileRecord: async (fileData: Omit<FichierImport, 'id' | 'created_at' | 'updated_at'>): Promise<FichierImport> => {
    console.log('Tentative de cr√©ation d\'un enregistrement de fichier avec les donn√©es:', JSON.stringify(fileData, null, 2));
    
    try {
      // Pr√©parer les donn√©es pour l'insertion en suivant exactement le sch√©ma
      const insertData = {
        // Champs obligatoires
        nom: fileData.nom,
        chemin: fileData.chemin,
        statut: 'actif', // Doit √™tre l'un de : 'actif', 'inactif', 'en_cours', 'erreur'
        
        // Champs avec valeurs par d√©faut dans la base de donn√©es
        date_import: new Date().toISOString(),
        nb_lignes: 0,
        nb_lignes_importees: 0,
        mapping_colonnes: {}, // JSONB non-null avec valeur par d√©faut {}
        separateur: ',',
        
        // Cl√© √©trang√®re obligatoire
        user_id: fileData.user_id,
        
        // Champs optionnels
        original_filename: fileData.original_filename || fileData.nom,
        taille: fileData.taille || null,
        type: fileData.type || null,
        mime_type: fileData.mime_type || null,
        
        // Champs non inclus pr√©c√©demment
        metadata: null, // Champ JSONB optionnel
        donnees: null // Champ JSONB optionnel pour les donn√©es du fichier
      };
      
      // V√©rification des contraintes
      if (!insertData.user_id) {
        throw new Error('user_id est obligatoire pour cr√©er un enregistrement de fichier');
      }

      console.log('Donn√©es simplifi√©es pour l\'insertion:', JSON.stringify(insertData, null, 2));
      
      // Essayer d'abord une insertion simple
      const { data, error } = await supabase
        .from('fichiers_import')
        .insert([insertData])
        .select()
        .single();

      if (error) {
        console.error('Erreur d√©taill√©e lors de la cr√©ation du fichier:', {
          message: error.message,
          code: error.code,
          details: error.details,
          hint: error.hint,
          error: JSON.stringify(error, Object.getOwnPropertyNames(error))
        });
        
        // Essayer d'obtenir plus d'informations sur les contraintes de la table
        try {
          const { data: tableInfo } = await supabase
            .rpc('get_table_info', { table_name: 'fichiers_import' })
            .single();
          console.log('Structure de la table:', tableInfo);
        } catch (infoError) {
          console.error('Impossible de r√©cup√©rer les informations de la table:', infoError);
        }
        
        throw error;
      }

      if (!data) {
        throw new Error('Aucune donn√©e retourn√©e lors de la cr√©ation du fichier');
      }

      console.log('Enregistrement cr√©√© avec succ√®s:', data);
      return data;
    } catch (error) {
      console.error('Erreur inattendue dans createFileRecord:', {
        error: error instanceof Error ? error.message : error,
        stack: error instanceof Error ? error.stack : undefined
      });
      throw error;
    }
  },

  /**
   * Met √† jour les donn√©es d'un fichier avec les leads pour d√©clencher le trigger
   */
  async updateFileWithLeadsData(fileId: string, leadsData: any[]): Promise<void> {
    const { error } = await supabase.rpc('update_file_with_leads_data', {
      p_file_id: fileId,
      p_leads_data: leadsData
    });
    
    if (error) throw error;
  },

  /**
   * Synchronise manuellement tous les leads depuis les fichiers
   */
  async syncAllLeads(): Promise<number> {
    try {
      const { data, error } = await supabase.rpc('manual_sync_all_leads');
      
      if (error) {
        console.warn('Fonction RPC manual_sync_all_leads non disponible:', error.message);
        throw error;
      }
      
      return data || 0;
    } catch (error) {
      console.error('Erreur lors de la synchronisation des leads:', error);
      throw error;
    }
  },

  /**
   * Synchronise les leads pour un fichier sp√©cifique
   */
  async syncFileLeads(fileId: string): Promise<number> {
    try {
      const { data, error } = await supabase.rpc('manual_sync_file_leads', { 
        p_file_id: fileId 
      });
      
      if (error) {
        console.warn('Fonction RPC manual_sync_file_leads non disponible:', error.message);
        throw error;
      }
      
      return data || 0;
    } catch (error) {
      console.error('Erreur lors de la synchronisation des leads du fichier:', error);
      throw error;
    }
  },

  /**
   * Met √† jour le statut d'un fichier
   */
  updateFileStatus: async (id: string, status: 'actif' | 'inactif' | 'en_cours' | 'erreur'): Promise<FichierImport> => {
    const { data, error } = await supabase
      .from('fichiers_import')
      .update({ 
        statut: status, 
        updated_at: new Date().toISOString() 
      })
      .eq('id', id)
      .select()
      .single();

    if (error) {
      console.error('Erreur lors de la mise √† jour du statut:', error);
      throw error;
    }

    return data;
  },

  restoreFile: async (id: string): Promise<FichierImport> => {
    const { data, error } = await supabase
      .from('fichiers_import')
      .update({
        statut: 'actif',
        updated_at: new Date().toISOString(),
        metadata: null
      })
      .eq('id', id)
      .select()
      .single();

    if (error) {
      console.error('Erreur lors de la restauration du fichier:', error);
      throw error;
    }

    return data;
  },

  /**
   * Supprime un fichier et toutes ses donn√©es associ√©es
   */
  deleteFile: async (id: string, filePath: string): Promise<void> => {
    try {
      console.log(`D√©but de la suppression du fichier ${id}`);
      
      // 1. D'abord supprimer les leads associ√©s si existants
      const { error: leadsError } = await supabase
        .from('leads')
        .delete()
        .eq('fichier_id', id);

      if (leadsError) {
        console.error('Erreur lors de la suppression des leads associ√©s:', leadsError);
        throw new Error(`Impossible de supprimer les leads: ${leadsError.message}`);
      }
      console.log('Leads supprim√©s avec succ√®s');

      // 2. Supprimer les logs de synchronisation (avant le fichier)
      const { error: syncLogsError } = await supabase
        .from('sync_logs')
        .delete()
        .eq('fichier_id', id);

      if (syncLogsError) {
        console.error('Erreur lors de la suppression des logs de synchronisation:', syncLogsError);
        throw new Error(`Impossible de supprimer les logs: ${syncLogsError.message}`);
      }
      console.log('Logs de synchronisation supprim√©s avec succ√®s');

      // 3. Supprimer les associations avec les campagnes
      const { error: campaignFilesError } = await supabase
        .from('campaign_file_links')
        .delete()
        .eq('fichier_id', id);

      if (campaignFilesError) {
        console.error('Erreur lors de la suppression des associations de campagne:', campaignFilesError);
        throw new Error(`Impossible de supprimer les associations: ${campaignFilesError.message}`);
      }
      console.log('Associations de campagne supprim√©es avec succ√®s');

      // 4. Supprimer le fichier du stockage
      if (filePath) {
        const { error: storageError } = await supabase.storage
          .from(BUCKET_NAME)
          .remove([filePath]);

        if (storageError) {
          console.error('Erreur lors de la suppression du fichier du stockage:', storageError);
          // Continuer m√™me si la suppression du stockage √©choue
          console.log('Continuation malgr√© l\'erreur de stockage');
        } else {
          console.log('Fichier du stockage supprim√© avec succ√®s');
        }
      }

      // 5. Supprimer d√©finitivement l'entr√©e de la table (suppression physique)
      const { error: dbError } = await supabase
        .from('fichiers_import')
        .delete()
        .eq('id', id);

      if (dbError) {
        console.error('Erreur lors de la suppression du fichier de la base:', dbError);
        throw new Error(`Impossible de supprimer le fichier: ${dbError.message}`);
      }

      console.log(`Fichier ${id} supprim√© avec succ√®s`);
    } catch (error) {
      console.error('Erreur lors de la suppression du fichier:', error);
      throw error;
    }
  },

  // ====================================
  // Gestion des colonnes personnalis√©es
  // ====================================

  /**
   * R√©cup√®re les colonnes personnalis√©es d'un utilisateur
   */
  getCustomColumns: async (userId: string): Promise<CustomColumn[]> => {
    try {
      const { data, error } = await supabase
        .from('user_custom_columns')
        .select('*')
        .eq('user_id', userId)
        .order('created_at', { ascending: true });

      if (error) {
        console.error('Erreur lors de la r√©cup√©ration des colonnes personnalis√©es:', {
          message: error.message,
          code: error.code,
          details: error.details,
          hint: error.hint,
        });
        throw error;
      }

      return data || [];
    } catch (error) {
      console.error('Erreur inattendue dans getCustomColumns:', error);
      throw error;
    }
  },

  /**
   * Ajoute une nouvelle colonne personnalis√©e
   */
  addCustomColumn: async (column: Omit<CustomColumn, 'id' | 'created_at' | 'updated_at'>): Promise<CustomColumn> => {
    const { data, error } = await supabase
      .from('user_custom_columns')
      .insert([{
        ...column,
        created_at: new Date().toISOString(),
        updated_at: new Date().toISOString()
      }])
      .select()
      .single();

    if (error) {
      console.error('Erreur lors de l\'ajout de la colonne personnalis√©e:', error);
      throw error;
    }

    return data;
  },

  /**
   * Met √† jour une colonne personnalis√©e
   */
  updateCustomColumn: async (
    columnId: string,
    updates: Partial<Omit<CustomColumn, 'id' | 'user_id' | 'created_at'>>
  ): Promise<CustomColumn> => {
    const { data, error } = await supabase
      .from('user_custom_columns')
      .update({
        ...updates,
        updated_at: new Date().toISOString()
      })
      .eq('id', columnId)
      .select()
      .single();

    if (error) {
      console.error('Erreur lors de la mise √† jour de la colonne personnalis√©e:', error);
      throw error;
    }

    return data;
  },

  /**
   * Supprime une colonne personnalis√©e
   */
  deleteCustomColumn: async (columnId: string): Promise<void> => {
    const { error } = await supabase
      .from('user_custom_columns')
      .delete()
      .eq('id', columnId);

    if (error) {
      console.error('Erreur lors de la suppression de la colonne personnalis√©e:', error);
      throw error;
    }
  },

  // ====================================
  // Traitement des fichiers
  // ====================================

  /**
   * Lit les en-t√™tes d'un fichier Excel
   */
  readFileHeaders: async (file: File): Promise<SheetInfo[]> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      
      reader.onload = (e) => {
        try {
          const data = new Uint8Array(e.target?.result as ArrayBuffer);
          const workbook = XLSX.read(data, { type: 'array' });
          const sheets: SheetInfo[] = [];

          workbook.SheetNames.forEach((sheetName) => {
            const worksheet = workbook.Sheets[sheetName];
            const jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1 });
            const headers = jsonData[0] as string[] || [];
            
            sheets.push({
              name: sheetName,
              rowCount: jsonData.length - 1, // Exclure l'en-t√™te
              headers: headers.map(h => h?.toString() || '')
            });
          });

          resolve(sheets);
        } catch (error) {
          reject(error);
        }
      };

      reader.onerror = (error) => reject(error);
      reader.readAsArrayBuffer(file);
    });
  },

  /**
   * Importe les donn√©es d'un fichier
   */
  importData: async (
    file: File, 
    mapping: Record<string, string>, 
    options?: { sheetIndex?: number, fileId?: string }
  ): Promise<{ rowCount: number }> => {
    // Lire et mapper les donn√©es du fichier
    const jsonData = await fileService.readFileAsJson(file, options?.sheetIndex);
    const mappedData = fileService.mapData(jsonData, mapping);
    
    // Mettre √† jour les m√©tadonn√©es du fichier dans la base de donn√©es
    if (options?.fileId) {
      try {
        await supabase
          .from('fichiers_import')
          .update({ 
            nb_lignes: jsonData.length,
            nb_lignes_importees: mappedData.length,
            mapping_colonnes: mapping,
            updated_at: new Date().toISOString()
          })
          .eq('id', options.fileId);
      } catch (updateError) {
        console.error('Erreur lors de la mise √† jour des m√©tadonn√©es du fichier:', updateError);
        // Ne pas √©chouer l'importation si la mise √† jour des m√©tadonn√©es √©choue
      }
    }
    
    // Ici, vous pouvez ajouter la logique pour enregistrer les donn√©es dans votre base de donn√©es
    // Par exemple :
    // const { error } = await supabase.from('leads').insert(mappedData);
    // if (error) throw error;
    
    return { rowCount: mappedData.length };
  },

  /**
   * Lit un fichier et le convertit en JSON
   */
  readFileAsJson: async (file: File, sheetIndex = 0): Promise<any[]> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      
      reader.onload = (e) => {
        try {
          const data = new Uint8Array(e.target?.result as ArrayBuffer);
          const workbook = XLSX.read(data, { type: 'array' });
          const sheetName = workbook.SheetNames[sheetIndex];
          const worksheet = workbook.Sheets[sheetName];
          const jsonData = XLSX.utils.sheet_to_json(worksheet, { header: 1 });
          
          // Convertir en tableau d'objets avec les en-t√™tes comme cl√©s
          if (jsonData.length < 2) {
            resolve([]);
            return;
          }
          
          const headers = (jsonData[0] as string[]).map(h => h?.toString() || '');
          const result = [];
          
          for (let i = 1; i < jsonData.length; i++) {
            const row = jsonData[i] as any[];
            const obj: Record<string, any> = {};
            
            headers.forEach((header, index) => {
              if (header && row[index] !== undefined) {
                obj[header] = row[index];
              }
            });
            
            if (Object.keys(obj).length > 0) {
              result.push(obj);
            }
          }
          
          resolve(result);
        } catch (error) {
          reject(error);
        }
      };
      
      reader.onerror = (error) => reject(error);
      reader.readAsArrayBuffer(file);
    });
  },

  /**
   * Mappe les donn√©es selon la configuration
   */
  mapData: (data: any[], mapping: Record<string, string>): any[] => {
    return data.map((row: any) => {
      const mappedRow: any = {};
      
      Object.entries(mapping).forEach(([source, target]) => {
        if (row[source] !== undefined) {
          mappedRow[target] = row[source];
        }
      });
      
      return mappedRow;
    });
  },

  /**
   * Met √† jour le mapping des colonnes d'un fichier
   */
  updateFileMapping: async (fileId: string, mapping: Record<string, string>): Promise<FichierImport> => {
    const { data, error } = await supabase
      .from('fichiers_import')
      .update({ 
        mapping_colonnes: mapping,
        updated_at: new Date().toISOString()
      })
      .eq('id', fileId)
      .select()
      .single();

    if (error) {
      console.error('Erreur lors de la mise √† jour du mapping:', error);
      throw error;
    }

    return data;
  },

  // ====================================
  // Utilitaires
  // ====================================

  /**
   * V√©rifie si un fichier existe dans le stockage
   */
  fileExists: async (filePath: string): Promise<boolean> => {
    const { data, error } = await supabase.storage
      .from(BUCKET_NAME)
      .list('', { 
        limit: 1,
        search: filePath
      });

    if (error) {
      console.error('Erreur lors de la v√©rification du fichier:', error);
      return false;
    }

    return data.length > 0;
  },

  /**
   * R√©cup√®re l'URL publique d'un fichier
   */
  getFilePublicUrl: (filePath: string): string | null => {
    const { data } = supabase.storage
      .from(BUCKET_NAME)
      .getPublicUrl(filePath);
    return data?.publicUrl || null;
  },

  /**
   * T√©l√©charge un fichier
   */
  downloadFile: async (filePath: string): Promise<Blob> => {
    const { data, error } = await supabase.storage
      .from(BUCKET_NAME)
      .download(filePath);

    if (error) {
      throw new Error(`Erreur lors du t√©l√©chargement: ${error.message}`);
    }

    return data;
  },

  /**
   * R√©cup√®re la liste des campagnes disponibles
   */
  getCampaigns: async (): Promise<any[]> => {
    try {
      const { data, error } = await supabase
        .from('campaigns')  // Correction: 'campaigns' au lieu de 'campagnes'
        .select('*')
        .order('name', { ascending: true });  // Correction: 'name' au lieu de 'nom'

      if (error) {
        throw new Error(`Erreur lors du chargement des campagnes: ${error.message}`);
      }

      return data || [];
    } catch (error) {
      console.error('Erreur dans getCampaigns:', error);
      throw error;
    }
  }
};

export type { FichierImport };
